# pip install streamlit python-dotenv langchain langchain-openai langchain-community sentence-transformers faiss-cpu langchain-upstage openai==1.* 
# Make sure openai version is >= 1.0
# -*- coding: utf-8 -*-
import os
import re
import streamlit as st
from dotenv import load_dotenv
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional
import itertools # ëª¨ë¸ ì¡°í•© ìƒì„±ìš©
import json # í‰ê°€ì…‹ íŒŒì¼ ë¡œë”©ìš©
import numpy as np # í‰ê·  ê³„ì‚°ìš©

# --- Langchain & AI Model Imports ---
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain import hub
from sentence_transformers import CrossEncoder # For Reranking
from openai import OpenAI # ìš”ì•½ & reranker & GPT Scoring
from langchain.prompts import PromptTemplate

#--- User Query Prompt ---
from query_prompt import build_user_query_prompt_chain, stream_final_answer_only

# --- langsmith  ---
from langchain.callbacks.tracers.langchain import LangChainTracer
from langchain.schema.messages import HumanMessage

# --- upstage ----
from langchain_upstage import UpstageGroundednessCheck

# --- RAGAS ----
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy
)

from datasets import Dataset
import pandas as pd

# --- Score ----
from sklearn.metrics import f1_score


# --- Const and setting ---
BASE_DIR = Path(__file__).resolve().parent            # ì‚¬ëŒë§ˆë‹¤ í˜„ì¬ í´ë” ì„¤ì • ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ ê·¸ì— ë”°ë¼ .parent ëª‡ê°œ ë¶™ì¼ì§€ ê²°ì •ì •
FAISS_DB_PATH = Path(BASE_DIR / "faiss_db") # ìœ„ì— êº¼ ë§ì¶°ì„œ faiss_db ê²½ë¡œë„ ë³€ê²½


# --- Load Env ---
load_dotenv(BASE_DIR / ".env")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
# Option for LangSmith tracing
# LANGCHAIN_API_KEY = os.getenv("LANGCHAIN_API_KEY")
# LANGCHAIN_PROJECT = os.getenv("LANGCHAIN_PROJECT")
# LANGCHAIN_TRACING_V2 = os.getenv("LANGCHAIN_TRACING_V2")

# --- Langsmith Setting ---
PROJECT_NAME = "RAG-System"
TRACER = LangChainTracer(project_name=PROJECT_NAME)

# --- Model Definitions ---
AVAILABLE_EMBEDDINGS = {
    "bge-m3": "BAAI/bge-m3",
    "e5e": "intfloat/e5-large-v2",
    "openai": "text-embedding-3-large"
}
AVAILABLE_RERANKERS = ["ko-rerank", "mini", "bge"]
# --- Default Settings ---
DEFAULT_EMBEDDING_ALIAS = "bge-m3"
DEFAULT_RERANKER_METHOD = "ko-rerank"
DEFAULT_RETRIEVER_K = 8
DEFAULT_RERANKER_TOP_K = 4
DEFAULT_LLM_MODEL_NAME = "gpt-4o"
DEFAULT_GPT_SCORING_MODEL = "gpt-4o" # "gpt-4"

# Document Formatting
# documentì—ì„œ ë‚´ìš©ë§Œ ë½‘ì•„ì„œ strí˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ -> LLM ë¶€ë¥¼ ë•Œ ì´ìš©
def format_docs(docs: List[Document]) -> str:
    return "\n\n".join(doc.page_content for doc in docs)


########################## RERANK functions ###############################
# GPT Document Summarization (Optional, used in summarize_and_rerank)
# GPTë¥¼ ì´ìš©í•œ retrive ë¬¸ì„œ 512í† í° ì´í•˜ ìš”ì•½ í•¨ìˆ˜. API ë¹„ìš© ë°œìƒ(Optional, gpt-3.5 ì“°ë„ë¡ ê³ ì •)
# Use Langsmith for trace
def gpt_summarize_document(doc: Document, max_tokens=512, api_key=None) -> Document:
    if not api_key:
        raise ValueError("OpenAI API key is required for summarization.")
    # Use LangChain wrapper?
    llm = ChatOpenAI(model="gpt-4o", max_tokens=max_tokens, openai_api_key=api_key, callbacks=[TRACER])
    prompt = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ {max_tokens}í† í° ì´í•˜ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\n\në¬¸ì„œ:\n{doc.page_content}"""

    try:
        # Use LangChain method
        response = llm.invoke([HumanMessage(content=prompt)])
        summary = response.content.strip()

        # Store original text in metadata
        new_metadata = doc.metadata.copy()
        new_metadata["original_text"] = doc.page_content
        return Document(page_content=summary, metadata=new_metadata)
    except Exception as e:
        st.error(f"ë¬¸ì„œ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # Return original document if fail
        return doc

#ì…ë ¥ ë°›ì€ reranker model ë°˜í™˜ í•¨ìˆ˜
#rerankëª¨ë¸ ë°›ì•„ì˜¤ëŠ” ê±¸ í•œë²ˆë§Œ í•˜ê³  ê·¸ ë’¤ì—” ìºì‹œì— ì €ì¥ëœê±¸ë¡œ ì´ìš©
@st.cache_resource(show_spinner=False) 
def get_reranker_model(method: str):
    method = method.lower()
    st.write(f"Reranker ëª¨ë¸ ë¡œë”© ì¤‘: {method}...")
    if method == "ko-rerank":
        return CrossEncoder("Dongjin-kr/ko-reranker", device='cpu', max_length=512)
    elif method == "bge":
        return CrossEncoder("BAAI/bge-reranker-base", device='cpu', max_length=512)
    elif method == "mini":
        return CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2", device='cpu', max_length=512)
    # gpt reranker ì„ì‹œ ì œê±°
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” reranker methodì…ë‹ˆë‹¤: {method}")

#Reranker ëª¨ë¸ ì‹¤í–‰ í•¨ìˆ˜
def rerank_documents(
    query: str,
    docs: List[Document],
    method: str,
    top_k: int = 3,
    openai_api_key: str = None # For future GPT reranker
) -> List[Tuple[Document, float]]:

    if method not in AVAILABLE_RERANKERS:
         st.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¦¬ë­ì»¤({method})ì…ë‹ˆë‹¤. ë¦¬ë­í‚¹ì„ ê±´ë„ˆ<0xEB><0>ëœë‹ˆë‹¤.")
         # Return original docs with dummy scores if method is invalid or None
         return [(doc, 1.0 / (i + 1)) for i, doc in enumerate(docs)][:top_k]

    try:
        reranker = get_reranker_model(method) # Load the specified model
        st.write(f"CrossEncoder ({method})ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì¬ìˆœìœ„ í‰ê°€ ì¤‘...")
        pairs = [(query, doc.page_content) for doc in docs]
        scores = reranker.predict(pairs)
        scored = list(zip(docs, scores))
        # Sort descending
        return sorted(scored, key=lambda x: x[1], reverse=True)[:top_k]
    except Exception as e:
        st.error(f"ë¦¬ë­í‚¹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({method}): {e}")
        # Fallback to returning top_k documents without reranking if error occurs
        return [(doc, 0.0) for doc in docs[:top_k]]

#ìš”ì•½ ë° Rerank í•¨ìˆ˜
def summarize_and_rerank(
    query: str,
    docs: List[Document],
    summarize_first: bool = False, # Controls summarization
    method: str = "bge",
    top_k: int = 3,
    openai_api_key: str = None
) -> List[Document]:

    docs_to_rerank = docs
    if summarize_first:
        if not openai_api_key:
             st.warning("ë¬¸ì„œ ìš”ì•½ì„ ìœ„í•´ì„œëŠ” OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        else:
            st.write("ë¬¸ì„œ ìš”ì•½ ì¤‘ (Reranking ì „)...")
            with st.spinner("ìš”ì•½ ì§„í–‰ ì¤‘..."):
                docs_to_rerank = [
                    gpt_summarize_document(doc, max_tokens=512, api_key=openai_api_key) for doc in docs
                ]

    # Perform reranking
    reranked_with_scores = rerank_documents(
        query=query,
        docs=docs_to_rerank,
        method=method,
        top_k=top_k,
        openai_api_key=openai_api_key # For future
    )

    final_original_docs = []
    st.write("ì¬ìˆœìœ„í™”ëœ ë¬¸ì„œ (ì ìˆ˜ ìˆœ):")
    for i, (doc, score) in enumerate(reranked_with_scores):
        # Retrieve the original content from metadata if summarized, otherwise use page_content
        original_content = doc.metadata.get("original_text", doc.page_content)
        # Create a new Document object with the original content but keep metadata
        final_doc = Document(page_content=original_content, metadata=doc.metadata)
        final_original_docs.append(final_doc)

        # Display reranked doc info (optional)
        st.write(f"  - Rank {i+1} (Score: {score:.4f}): ", final_doc.metadata.get('file_name', 'N/A'), f" (Index: {final_doc.metadata.get('index', 'N/A')})")

    return final_original_docs



########  Vector DB function  #########

# Cache the loaded vector store
# ì„ë² ë”© ëª¨ë¸ ì„ íƒí•˜ê³  ëª¨ë¸ì— ë”°ë¥¸ DB ë¶ˆëŸ¬ì˜¤ê¸° -> openai ì„ë² ë”© ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ”ê±´ ì¶”ì  ì•ˆë¨
@st.cache_resource(show_spinner="ë²¡í„° DB ë¡œë”© ì¤‘...") #DB ë°›ì•„ì˜¤ëŠ” ê±¸ í•œë²ˆë§Œ í•˜ê³  ê·¸ ë’¤ì—” ìºì‹œì— ì €ì¥ëœê±¸ë¡œ ì´ìš©
def load_vector_store(faiss_alias, api_key):

    # Call embedding models
    ##### ì„ë² ë”© ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” íŒŒíŠ¸ #####
    if faiss_alias not in AVAILABLE_EMBEDDINGS:
        st.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© ëª¨ë¸ ë³„ì¹­ì…ë‹ˆë‹¤: {faiss_alias}")
        return None
    model_name = AVAILABLE_EMBEDDINGS[faiss_alias]

    st.write(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {faiss_alias} ({model_name})...")
    embedding_model = None
    if faiss_alias == "openai":
        # OpenAI API ì‚¬ìš©
        if not api_key:
            st.error("OpenAI ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•˜ë ¤ë©´ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return None
        embedding_model = OpenAIEmbeddings(model=model_name, openai_api_key=api_key)
        st.write("(OpenAI ì„ë² ë”© ì‚¬ìš© ì‹œ API ë¹„ìš©ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    else:
        # ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
        try:
            # device = "cpu"
            device = "cpu" # device = "cuda" if torch.cuda.is_available()
            embedding_model = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs={"device": device},
                encode_kwargs={"normalize_embeddings": True}
            )
        except Exception as e:
            st.error(f"HuggingFace ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ({model_name}): {e}")
            st.error("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”: pip install sentence-transformers torch")
            return None

    ##### localì— ì €ì¥ëœ ë²¡í„° DB ë¶ˆëŸ¬ì˜¤ëŠ” íŒŒíŠ¸ #####

    # full path to the FAISS index directory
    db_path = FAISS_DB_PATH / faiss_alias
    index_file_faiss = db_path / "index.faiss"
    index_file_pkl = db_path / "index.pkl"

    if not index_file_faiss.exists() or not index_file_pkl.exists():
        st.error(f"FAISS ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {db_path}")
        st.error(f"ë¨¼ì € '{faiss_alias}' ëª¨ë¸ì— ëŒ€í•œ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•˜ê³  í•´ë‹¹ ê²½ë¡œì— ì €ì¥í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return None

    try:
        vectorstore = FAISS.load_local(
            folder_path=str(db_path), # Pass path as string
            embeddings=embedding_model,
            index_name="index", # Default index name used in saving
            allow_dangerous_deserialization=True # Needed for HuggingFaceEmbeddings
        )
        st.success(f"'{faiss_alias}' ë²¡í„° DB ë¡œë“œ ì™„ë£Œ!")
        return vectorstore
    except Exception as e:
        st.error(f"FAISS ë²¡í„° DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({faiss_alias}): {e}")
        return None




###########  Factchecker function  ##########

# Upstage Fact checkerì— ë„˜ê¸¸ ë¼ë²¨
label_to_score={
    "grounded":1.0,
    "notSure":0.5,
    "notGrounded":0.0
}

# Rewrite User query -> Use "gpt-4o"
def rewrite_question_single(original_question: str, temperature: float = 0.3, api_key=None) -> str:
    if not api_key:
        st.error("ì§ˆë¬¸ ì¬ì‘ì„±ì„ ìœ„í•´ì„œëŠ” OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return original_question # Return original if key is missing

    prompt_template = """
    ë‹¹ì‹ ì€ ì •ë³´ ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ ì§ˆë¬¸ ë¦¬ë¼ì´íŒ… ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
    ë‹¤ìŒ ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ì´ê³  ê²€ìƒ‰ì— ì í•©í•˜ê²Œ ë°”ê¿”ì£¼ì„¸ìš”.
    ë‹¨, ì›ë˜ ì§ˆë¬¸ì— ìˆë˜ ì‘ë¬¼ì€ ë˜‘ê°™ì´ ìœ ì§€í•´ì£¼ì„¸ìš”.
    í•œ ê°€ì§€ í˜•íƒœë¡œë§Œ ë³€í™˜í•´ ì£¼ì„¸ìš”.
    ì›ë˜ ì§ˆë¬¸: "{original_question}"
    """
    prompt = PromptTemplate.from_template(prompt_template)
    llm = ChatOpenAI(model="gpt-4o", temperature=temperature, openai_api_key=api_key, callbacks=[TRACER])
    chain = prompt | llm

    try:
        response = chain.invoke({"original_question": original_question})
        return response.content
    except Exception as e:
        st.error(f"ì§ˆë¬¸ ì¬ì‘ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return original_question # If error, return original

# ë¬¸ì„œì—ì„œ ì›í•˜ëŠ” ë‚´ìš© íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜
def sentence_split(text):
    # If text is a string?
    if not isinstance(text, str):
        return []
    split_sentence=re.split(r'(?<=[-.!?])\s+',text)
    # Filter out empty strings that might result from splitting
    return [s for s in split_sentence if s]

# Upstage Fact checker function -> Use UpStage API 
def fact_checker(sentences: List[str], retrieved_docs: List[Document], api_key: str) -> Tuple[float, List[float]]:
    if not api_key:
        st.warning("Fact Checkingì„ ìœ„í•´ì„œëŠ” Upstage API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. ê¸°ë³¸ ì ìˆ˜ 0.0ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return 0.0, [0.0] * len(sentences)

    results = []
    try:
        groundedness_check = UpstageGroundednessCheck(api_key=api_key) # Pass API key
    except Exception as e:
        st.error(f"Upstage GroundednessCheck ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. Fact Checkingì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return 0.0, [0.0] * len(sentences)

    st.write(f"ë¬¸ì¥ {len(sentences)}ê°œì— ëŒ€í•´ Fact Checking ìˆ˜í–‰ ì¤‘... (Upstage API ë¹„ìš© ë°œìƒ)")
    for i, sentence in enumerate(sentences):
        if not sentence:
            results.append(0.0)
            continue

        best_score_for_sentence = 0.0
        for doc in retrieved_docs:
            chunk = doc.page_content
            request_input = {"context": chunk, "answer": sentence}
            try:
                res = groundedness_check.invoke(request_input)
                score = label_to_score.get(res, 0.0)
                if score > best_score_for_sentence:
                    best_score_for_sentence = score
                    if best_score_for_sentence == 1.0: break
            except Exception as e:
                st.warning(f"Upstage API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬¸ì¥: '{sentence[:30]}...'): {e}")
                best_score_for_sentence = 0.0
                break
        results.append(best_score_for_sentence)

    if not results: return 0.0, []
    average_score = sum(results) / len(results)
    return average_score, results



#################   GPT scoring  #################
#LLM + RAGAS íŒŒì‹± í•¨ìˆ˜
def parse_combined_llm_scores(text: str) -> Dict[str, float]:
    pattern = r"(Intent Understanding|Semantic Similarity|Score Reliability):\s*([1-5])"
    matches = re.findall(pattern, text)
    return {label: int(score) / 5 for label, score in matches}

#LLM + RAGAS ìŠ¤ì½”ì–´ í‰ê°€ í•¨ìˆ˜
def get_combined_score(query: str, response: str, context: str, ground_truth: str, api_key: str) -> Dict[str, Any]:
    # 1. RAGAS í‰ê°€
    ragas_dataset = Dataset.from_dict({
        "question": [query],
        "response": [response],
        "retrieved_contexts": [[context]],
        "reference": [ground_truth]
    })

    ragas_result = evaluate(ragas_dataset, metrics=[faithfulness, answer_relevancy])
    ragas_df = ragas_result.to_pandas() 

    ragas_scores = {
        "faithfulness": round(ragas_df.loc[0, "faithfulness"], 3),
        "answer_relevancy": round(ragas_df.loc[0, "answer_relevancy"], 3)
    }

    # 2. LLM í‰ê°€ í”„ë¡¬í”„íŠ¸
    prompt = f"""
[ì§ˆë¬¸]
{query}

[Ground Truth]
{ground_truth}

[RAG ì‹œìŠ¤í…œì˜ ì‘ë‹µ]
{response}

ë‹¹ì‹ ì€ ë†ì‚°ë¬¼ RAG ì‹œìŠ¤í…œì˜ ì‘ë‹µì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì„¸ ê°€ì§€ í•­ëª©ì— ëŒ€í•´ ê°ê° 1~5ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•´ì£¼ì„¸ìš”.  
ê°€ëŠ¥í•œ í•œ **ê°ê´€ì **ì´ê³  ì¼ê´€ëœ ê¸°ì¤€ì— ë”°ë¼ í‰ê°€í•´ ì£¼ì„¸ìš”.

---

### 1. Intent Understanding  
ì‘ë‹µì´ ì§ˆë¬¸ìì˜ **ì˜ë„ë¥¼ ì˜ ì´í•´í–ˆëŠ”ì§€** í‰ê°€í•˜ì„¸ìš”.

- 5ì : ì§ˆë¬¸ì— ì •í™•íˆ ë‹µí•˜ê³ , ì§ˆë¬¸ì ì˜ë„ê¹Œì§€ íŒŒì•…í•˜ì—¬ 
    ì˜ˆì‹œ : ë³‘ì¶©í•´ì˜ ì¢…ë¥˜ì™€ íŠ¹ì§•ì— ëŒ€í•´ ì„¤ëª…í•˜ê³ , ë°©ì œë²•ì— ëŒ€í•´ ì„¤ëª…í•˜ê±°ë‚˜ ì‚¬ìš©ìì—ê²Œ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•œì§€ ë¬¼ì–´ë´„.
- 3ì : ì§ˆë¬¸ì—ëŠ” ë‹µí–ˆì§€ë§Œ, ì§ˆë¬¸ìì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ì§€ ëª»í•¨
    ì˜ˆì‹œ : ë³‘ì¶©í•´ì˜ ì¢…ë¥˜ì™€ íŠ¹ì§•ì— ëŒ€í•´ì„œ ì„¤ëª…í–ˆì§€ë§Œ, ì–´ë–»ê²Œ ë°©ì œí•´ì•¼í•˜ëŠ”ì§€ ì„¤ëª…í•˜ì§€ ì•Šê³  ë˜í•œ ì‚¬ìš©ìì—ê²Œ ì¶”ê°€ì •ë³´ê°€ í•„ìš”í•œì§€ ë¬¼ì–´ë³´ì§€ ì•ŠìŒ.
- 1ì : ì§ˆë¬¸ì„ ì˜¤í•´í•˜ê±°ë‚˜ ì§ˆë¬¸ì˜ ë‚´ìš©ê³¼ ë¬´ê´€í•œ ì‘ë‹µ, ë˜ëŠ” ì˜¤íƒˆì í¬í•¨  
    ì˜ˆì‹œ : ë³‘ì¶©í•´ì— ëŒ€í•´ ë¬¼ì–´ë³´ì•˜ëŠ”ë° ì¬ë°°ë°©ë²•ì— ëŒ€í•´ ì‘ë‹µë‹µ
ë˜í•œ, "ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ì²˜ëŸ¼ ì •ì§í•˜ê²Œ íšŒí”¼í•œ ê²½ìš°ë„ ë†’ì€ ì ìˆ˜

---

### 2. Semantic Similarity  
Ground Truthì™€ ì‘ë‹µì´ **ë‚´ìš© ë° ì˜ë¯¸ ë©´ì—ì„œ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€** í‰ê°€í•˜ì„¸ìš”.

- 5ì : ì˜ë¯¸ëŠ” ê°™ê³  í‘œí˜„ë§Œ ë‹¤ë¦„
- 3ì : ì¼ë¶€ë§Œ ìœ ì‚¬í•˜ê±°ë‚˜ ìš”ì•½ ìˆ˜ì¤€
- 1ì : ë…¼ì§€ë‚˜ ì •ë³´ê°€ ì „í˜€ ë‹¤ë¦„

---

### 3. Score Reliability  
ë‹¹ì‹ ì´ ì§€ê¸ˆ í‰ê°€í•œ ì ìˆ˜ë“¤ì´ **ì–¼ë§ˆë‚˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”ì§€** ìŠ¤ìŠ¤ë¡œ í‰ê°€í•˜ì„¸ìš”.  
ì§ˆë¬¸/ë¬¸ì„œ/ì‘ë‹µì´ ëª…í™•í•˜ì—¬ í‰ê°€ê°€ ì‰¬ì› ë‹¤ë©´ ë†’ì€ ì ìˆ˜,  
ì •ë³´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì• ë§¤í–ˆë‹¤ë©´ ë‚®ì€ ì ìˆ˜.

---

### ì¶œë ¥ í˜•ì‹ (ìˆ«ìë§Œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”):

Intent Understanding: 4  
Semantic Similarity: 5  
Score Reliability: 5
"""
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0, openai_api_key=api_key, callbacks=[TRACER])
    
    llm_output = llm.invoke([HumanMessage(content=prompt)]).content
    llm_scores = parse_combined_llm_scores(llm_output)

    # 3. í•©ì³ì„œ ë°˜í™˜
    return {
        **llm_scores,
        **ragas_scores
    }







##########################   LLM ë‹µë³€ ìƒì„±ê¹Œì§€ì˜ Pipeline function  ###############################

# íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ : RAG ì‹¤í–‰ ë° LLM ë‹µë³€ ìƒì„±í•˜ëŠ” ê²ƒ ê¹Œì§€ ë‹´ë‹¹ -> streamlit UIì—ì„œ LLM ëª¨ë¸ ì„ íƒí•˜ëŠ” ë¶€ë¶„ì´ ë°˜ì˜ë˜ëŠ” ê³³
# OpenAPI API ì‚¬ìš©
# Langsmith ì¶”ì 
def run_rag_pipeline(
    question: str,
    vectorstore: FAISS,
    retriever_k: int,
    use_reranker: bool,
    reranker_method: Optional[str], # can None
    reranker_top_k: int,
    summarize_before_rerank: bool,
    llm_model_name: str,
    openai_api_key: str,
    run_id: str = ""
) -> Tuple[str, List[Document]]:
    """Run the Retrieve-Rerank(Optional)-Generate pipeline."""

    log_prefix = f"[{run_id}] " if run_id else ""

    # 1. Retrieve
    st.write(f"{log_prefix}1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ (k={retriever_k})...")
    retriever = vectorstore.as_retriever(search_kwargs={"k": retriever_k})
    initial_docs = retriever.invoke(question)
    st.write(f"{log_prefix}   ì´ˆê¸° ê²€ìƒ‰ëœ ë¬¸ì„œ {len(initial_docs)}ê°œ")

    # 2. Rerank (Optional)
    final_docs = []
    if use_reranker and reranker_method: # Check if method is provided
        st.write(f"{log_prefix}2. ë¬¸ì„œ rerank ì¤‘ (Top {reranker_top_k}, Method: {reranker_method})...")
        final_docs = summarize_and_rerank(
             query=question,
             docs=initial_docs,
             summarize_first=summarize_before_rerank,
             method=reranker_method,
             top_k=reranker_top_k,
             openai_api_key=openai_api_key
         )
        if not final_docs:
             st.warning(f"{log_prefix}   rerank í›„ ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ ì¼ë¶€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
             final_docs = initial_docs[:reranker_top_k]
    elif use_reranker and not reranker_method:
         st.warning(f"{log_prefix}   reranker ì‚¬ìš©ì´ ì„ íƒë˜ì—ˆìœ¼ë‚˜ ëª¨ë¸ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. rerankingì„ ê±´ë„ˆ<0xEB>ëœë‹ˆë‹¤.")
         final_docs = initial_docs[:reranker_top_k]
    else: # No reranking requested or no method specified
        st.write(f"{log_prefix}2. reranker ì‚¬ìš©í•˜ì§€ ì•ŠìŒ. ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš© (ìƒìœ„ {reranker_top_k}ê°œ).")
        final_docs = initial_docs[:reranker_top_k]

    if not final_docs:
         st.error(f"{log_prefix}   ë‹µë³€ ìƒì„±ì— ì‚¬ìš©í•  ë¬¸ì„œë¥¼ ì–»ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
         return "ì˜¤ë¥˜: ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ê±°ë‚˜ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []


    # 3. Generate (LLM í˜¸ì¶œ)
    st.write(f"{log_prefix}3. AI ë‹µë³€ ìƒì„± ì¤‘ ({llm_model_name})... ")

    #################################################################
    # Few-shot-CoT ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ì²´ì¸ ì‚¬ìš©
    try:
        context = format_docs(final_docs)
        full_question = f"{question}\n\nì°¸ê³  ë¬¸ì„œ:\n{context}"

        cot_chain = build_user_query_prompt_chain()
        result = cot_chain.invoke({"question": full_question})

        # content í•„ë“œë§Œ ì¶”ì¶œ (ë¶ˆí•„ìš”í•œ ë©”íƒ€ë°ì´í„° ì œê±°)
        if isinstance(result, dict) and "content" in result:
            content = result["content"]
        elif hasattr(result, "content"):
            content = result.content
        else:
            content = str(result)

        return content, final_docs

    except Exception as e:
        st.error(f"{log_prefix}   LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return f"ì˜¤ë¥˜: ë‹µë³€ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ ({e})", final_docs
    #################################################################

    # Define LLM
    llm = ChatOpenAI(model_name=llm_model_name, temperature=0, openai_api_key=openai_api_key, callbacks=[TRACER])

    # Define the RAG chain using the reranked context
    rag_chain = (
        # Pass the question and the pre-fetched/reranked context
        {"context": lambda x: context, "question": RunnablePassthrough()}
        | prompt_hub
        | llm
        | StrOutputParser()
    ).with_config({"callbacks": [TRACER]})# For callback tracing

    try:
        response = rag_chain.invoke(question)
        st.write(f"{log_prefix}   ë‹µë³€ ìƒì„± ì™„ë£Œ.")
        return response, final_docs # Return both response and the docs used for it
    except Exception as e:
        st.error(f"{log_prefix}   LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return f"ì˜¤ë¥˜: ë‹µë³€ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ ({e})", final_docs





############################## Evaluation Set Handling í•¨ìˆ˜ ################################
def load_evaluation_set(uploaded_file) -> Optional[List[Dict[str, Any]]]:
    if uploaded_file is None:
        st.error("í‰ê°€ì…‹ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return None

    try:
        uploaded_file.seek(0)
        eval_data = json.load(uploaded_file)

        required_keys = {'question', 'answer'}

        if not isinstance(eval_data, list):
            st.error("í‰ê°€ì…‹ì€ JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
            return None

        valid_data = []

        for item in eval_data:
            if isinstance(item, dict) and required_keys.issubset(item):
                # name ë˜ëŠ” numì´ ë¹„ì–´ ìˆì„ ê²½ìš° nameì„ "unrelate"ë¡œ ì„¤ì •
                is_name_empty = not item.get("name")
                is_num_empty = not item.get("num")

                if is_name_empty or is_num_empty:
                    item["name"] = ["unrelate"]  # ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ìœ ì§€

                valid_data.append(item)

        if len(valid_data) == 0:
            st.error("ìœ íš¨í•œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. 'question', 'answer', 'p', 'num', 'no', 'name' í‚¤ë¥¼ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.")
            return None

        st.success(f"í‰ê°€ì…‹ ë¡œë“œ ì™„ë£Œ: {len(valid_data)}ê°œ ì§ˆë¬¸")
        return valid_data

    except json.JSONDecodeError:
        st.error("JSON íŒŒì¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None
    except Exception as e:
        st.error(f"í‰ê°€ì…‹ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return None


############################## F1 Score ê³„ì‚° í•¨ìˆ˜ #############################

def compute_evalset_f1(
    evaluation_item: Dict[str, Any],
    used_docs: List[Document]
) -> float:
    """
    ë‹¨ì¼ í‰ê°€ í•­ëª©ê³¼ ì‚¬ìš© ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ê°„ F1 Score ê³„ì‚°
    :param evaluation_item: í•˜ë‚˜ì˜ í‰ê°€ í•­ëª© (ë”•ì…”ë„ˆë¦¬)
    :param used_docs: LangChain Document ë¦¬ìŠ¤íŠ¸ (metadata ê¸°ë°˜)
    :return: F1 Score (float)
    """
    file_to_faq = {
        "filtered_plants1.json": "FAQ1",
        "filtered_plants2.json": "FAQ2"
    }

    # í‰ê°€ í•­ëª©ì´ "unrelate"ì´ë©´ ì œì™¸
    if evaluation_item.get("name") == ["unrelate"]:
        return 0.0

    # í‰ê°€ í•­ëª© ì •ë³´ ì¶”ì¶œ
    ref_name = evaluation_item.get("name")[0] if isinstance(evaluation_item.get("name"), list) else evaluation_item.get("name")
    raw_nums = evaluation_item.get("num", [])
    ref_nums = [str(n) for n in raw_nums] if isinstance(raw_nums, list) else [str(raw_nums)]
    ref_plant = evaluation_item.get("p")[0] if isinstance(evaluation_item.get("p"), list) else evaluation_item.get("p")

    # ì‚¬ìš©ëœ ë¬¸ì„œ metadata ê¸°ì¤€ ë¹„êµ
    used_doc_metadata = [doc.metadata for doc in used_docs]

    match_found = False
    for meta in used_doc_metadata:
        file_name = meta.get("file_name", "")
        mapped_name = file_to_faq.get(file_name, file_name)
        index = str(meta.get("index"))
        plant = meta.get("plant")

        if mapped_name in ["FAQ1", "FAQ2"]:
            if (mapped_name == ref_name) and (index in ref_nums) and (plant == ref_plant):
                match_found = True
                break
        else:
            if (plant == ref_plant) and (index in ref_nums) and (mapped_name == ref_name):
                match_found = True
                break

    # ë‹¨ì¼ í•­ëª© ê¸°ì¤€ ì •ë‹µ 1, ì˜ˆì¸¡ì´ ë§ìœ¼ë©´ 1 ì•„ë‹ˆë©´ 0
    y_true = [1]
    y_pred = [1 if match_found else 0]

    return f1_score(y_true, y_pred)




##########################  Streamli UI  #################################

# --- Streamlit App UI ---
st.set_page_config(page_title="RAG ì‹œìŠ¤í…œ (ë†ì‚°ë¬¼ QA)", layout="wide")
st.title("RAG ê¸°ë°˜ ë†ì‚°ë¬¼ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")

# --- Sidebar for SEtting ---
st.sidebar.title("RAG ì„¤ì •")
st.sidebar.caption("ë¹„ìš© ì£¼ì˜!")

# Model Selection for single run
st.sidebar.header("ê¸°ë³¸ ì‹¤í–‰ ëª¨ë¸")
selected_embedding_alias = st.sidebar.selectbox(
    "ì„ë² ë”© ëª¨ë¸ ì„ íƒ",
    options=list(AVAILABLE_EMBEDDINGS.keys()),
    index=list(AVAILABLE_EMBEDDINGS.keys()).index(DEFAULT_EMBEDDING_ALIAS), # Default
    help="OpenAI ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© ì‹œ API ë¹„ìš©ì´ ë°œìƒ."
)

# Reranker Selection for single run
use_reranker_default = st.sidebar.checkbox("ë¦¬ë­ì»¤ ì‚¬ìš©", value=True, help="ë‹µë³€ ìƒì„± ì „ ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ìˆœìœ„ë¥¼ ì¬ì¡°ì •.")
selected_reranker_method_default = DEFAULT_RERANKER_METHOD # Default
if use_reranker_default:
    selected_reranker_method_default = st.sidebar.selectbox(
        "ë¦¬ë­ì»¤ ëª¨ë¸ ì„ íƒ",
        options=AVAILABLE_RERANKERS,
        index=AVAILABLE_RERANKERS.index(DEFAULT_RERANKER_METHOD) # Default
    )
else:
     selected_reranker_method_default = None # No reranker if checkbox is off


# LLM Selection
selected_llm = st.sidebar.selectbox(
    "LLM ëª¨ë¸ ì„ íƒ",
    options=["gpt-3.5-turbo", "gpt-4", "gpt-4o"], #ì¶”í›„ ì¶”ê°€
    index=0, # Default : gpt-3.5-turbo
    help="ë‹µë³€ ìƒì„±ì— ì‚¬ìš©ë  ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. API ë¹„ìš© ë°œìƒ."
)

# Parameter Tuning
st.sidebar.header("ê²€ìƒ‰/ì¬ìˆœìœ„ íŒŒë¼ë¯¸í„°")
retriever_k_value = st.sidebar.slider("ì´ˆê¸° ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ (Retriever K)", min_value=1, max_value=20, value=DEFAULT_RETRIEVER_K)
reranker_top_k_value = st.sidebar.slider("ìµœì¢… ë¬¸ì„œ ìˆ˜ (Reranker Top K)", min_value=1, max_value=10, value=DEFAULT_RERANKER_TOP_K, help="ë¦¬ë­ì»¤ ì‚¬ìš© ì‹œ ìµœì¢…ì ìœ¼ë¡œ LLMì—ê²Œ ì „ë‹¬ë  ë¬¸ì„œì˜ ìˆ˜.")

# Optional Features Toggles
st.sidebar.header("ë¶€ê°€ ê¸°ëŠ¥ (API ì‚¬ìš©)")
summarize_before_rerank_toggle = st.sidebar.checkbox(
    "ë¦¬ë­í‚¹ ì „ ë¬¸ì„œ ìš”ì•½ (OpenAI API)",
    value=False,
    help="ë¦¬ë­ì»¤ê°€ ì²˜ë¦¬í•˜ê¸° ì „ì— ê° ë¬¸ì„œë¥¼ OpenAIë¥¼ ì´ìš©í•´ ìš”ì•½. API ë¹„ìš©ì´ ë°œìƒ."
)
use_fact_checker_toggle = st.sidebar.checkbox(
    "Fact Checker ì‚¬ìš© (Upstage API)",
    value=True,
    help="ë‹µë³€ ìƒì„± í›„ Upstage Groundedness Checkë¥¼ ìˆ˜í–‰. ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ OpenAIë¥¼ ì´ìš©í•´ 1íšŒ ì§ˆë¬¸ ì¬ì‘ì„±ì„ ì‹œë„. Upstage API ë¹„ìš©ì´ ë°œìƒ."
)
use_gpt_scoring_toggle = st.sidebar.checkbox(
    f"GPT ìë™ í‰ê°€ (OpenAI API)({DEFAULT_GPT_SCORING_MODEL})",
    value=False,
    help=f"ìƒì„±ëœ ë‹µë³€ì— ëŒ€í•´ ({DEFAULT_GPT_SCORING_MODEL})ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ì„±, ì¶©ì‹¤ë„ ë“±ì„ í‰ê°€. API ë¹„ìš©ì´ ë°œìƒ."
)




###############################     ë©”ì¸ ì½”ë“œ     #################################

# --------- Main Area ---------
st.caption(f"í˜„ì¬ ì„¤ì • | Embedding: {selected_embedding_alias} | Reranker: {'ì‚¬ìš© ì•ˆ í•¨' if not use_reranker_default else selected_reranker_method_default} | LLM: {selected_llm}")
st.caption(f"ë¶€ê°€ ê¸°ëŠ¥ | ìš”ì•½: {'í™œì„±' if summarize_before_rerank_toggle else 'ë¹„í™œì„±'} | FactCheck: {'í™œì„±' if use_fact_checker_toggle else 'ë¹„í™œì„±'} | GPT í‰ê°€: {'í™œì„±' if use_gpt_scoring_toggle else 'ë¹„í™œì„±'}")

# Load the primary vector store based on sidebar selection
# Cache the vector store based on the selected alias
@st.cache_resource(show_spinner="ì„ íƒëœ ë²¡í„° DB ë¡œë”© ì¤‘...")
def get_cached_vector_store(alias, key):
     # Wrap the original load function for caching with alias as key
     return load_vector_store(alias, key)

vectorstore = get_cached_vector_store(selected_embedding_alias, OPENAI_API_KEY)

if vectorstore:
    # Question Input
    question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ë°°ì¶”ì˜ ì£¼ìš” ë³‘ì¶©í•´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?")

    # --- Single Run Button ---
    if st.button("ì§ˆë¬¸í•˜ê¸° (í˜„ì¬ ì„¤ì • ì‚¬ìš©)") and question:
        st.markdown("---")
        st.header("ë‹¨ì¼ ì‹¤í–‰ ê²°ê³¼")
        final_response = "ì˜¤ë¥˜: ì²˜ë¦¬ ì¤‘ ë¬¸ì œ ë°œìƒ"
        final_docs_used = []
        average_score = None # For fact checker

        try:
            # Indicate potential costs before running
            cost_flags = []
            if selected_embedding_alias == 'openai': cost_flags.append("OpenAI Embedding")
            if summarize_before_rerank_toggle: cost_flags.append("OpenAI Summarization")
            if selected_llm.startswith('gpt'): cost_flags.append(f"OpenAI LLM ({selected_llm})")
            if use_fact_checker_toggle: cost_flags.append("Upstage FactCheck")
            if use_gpt_scoring_toggle: cost_flags.append("OpenAI Scoring")
            if cost_flags:
                 st.info(f"API ë¹„ìš© ë°œìƒ ê°€ëŠ¥: {', '.join(cost_flags)}")
            
            
            # ì‹¤í—˜ìš© CoT ì¶œë ¥
            query_chain = build_user_query_prompt_chain()
            streamed = stream_final_answer_only(query_chain, question)

            st.markdown("### ğŸŒ± í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ GPT ì‘ë‹µ:")

            response_placeholder = st.empty()  # ë¹ˆ ê³µê°„ í•˜ë‚˜ ë§Œë“¤ê³ 
            response_text = ""
 
            for token in streamed:
                response_text += token
                response_placeholder.markdown(response_text)
            # ------------------------------------------------


            with st.spinner("RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘..."):
                final_response, final_docs_used = run_rag_pipeline(
                    question=question,
                    vectorstore=vectorstore,
                    retriever_k=retriever_k_value,
                    use_reranker=use_reranker_default,
                    reranker_method=selected_reranker_method_default,
                    reranker_top_k=reranker_top_k_value,
                    summarize_before_rerank=summarize_before_rerank_toggle,
                    llm_model_name=selected_llm,
                    openai_api_key=OPENAI_API_KEY
                )

            # --- Optional: Fact Checking ---
            if use_fact_checker_toggle and final_docs_used:
                 with st.spinner("4. Fact Checking ì¤‘... (Upstage API)"):
                     sentences = sentence_split(final_response)
                     if sentences: # Only run if there are sentences
                         average_score, _ = fact_checker(sentences, final_docs_used, UPSTAGE_API_KEY)
                         st.info(f"Fact Check Score: {average_score:.4f}")

                         # Rewrite logic based on score (Uses OpenAI API)
                         if (average_score > 0.3) & (average_score < 0.7):
                             st.warning("Fact Check ì ìˆ˜ê°€ ë‚®ì•„ ì§ˆë¬¸ì„ ì¬ì‘ì„±í•˜ì—¬ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤. (OpenAI API ì‚¬ìš©)")
                             rewritten_question = rewrite_question_single(question, api_key=OPENAI_API_KEY)
                             if rewritten_question != question: # Check if rewrite actually happened
                                 st.write(f"   ì¬ì‘ì„±ëœ ì§ˆë¬¸: {rewritten_question}")
                                 with st.spinner("ì¬ì‘ì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ RAG íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰ ì¤‘..."):
                                     # Re-run the pipeline with the rewritten question
                                     final_response, final_docs_used = run_rag_pipeline(
                                         question=rewritten_question, # Use rewritten question here
                                         vectorstore=vectorstore,
                                         retriever_k=retriever_k_value,
                                         use_reranker=use_reranker_default,
                                         reranker_method=selected_reranker_method_default,
                                         reranker_top_k=reranker_top_k_value,
                                         summarize_before_rerank=summarize_before_rerank_toggle,
                                         llm_model_name=selected_llm,
                                         openai_api_key=OPENAI_API_KEY,
                                         run_id="Rewrite" # Add identifier
                                     )
                                     # Optionally re-run fact-checker on the new response?
                             else:
                                st.warning("   ì§ˆë¬¸ ì¬ì‘ì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì§ˆë¬¸ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                     else:
                          st.write("   Fact Check ì ìˆ˜ê°€ ì„ê³„ì¹˜ ë²”ìœ„ ë°–ì´ê±°ë‚˜ ì–‘í˜¸í•©ë‹ˆë‹¤.")
                     # Display final fact check score if calculated
                     if average_score is not None:
                          st.caption(f"Upstage Fact Check í‰ê·  ì ìˆ˜: {average_score:.4f}")

            # Display Final Results
            st.markdown("### AI ì‘ë‹µ:")
            st.write(final_response)

            # --- Optional: GPT Scoring ---
            if use_gpt_scoring_toggle and final_docs_used:
                with st.spinner(f"5. GPT Scoring ì¤‘... ({DEFAULT_GPT_SCORING_MODEL})"):
                     #ì´ê±° ì´ì „ì— F1 score ë½‘ì•„ì•¼ í•œë‹¤.
                     
                     context_str_for_scoring = format_docs(final_docs_used)
                     
                     combined_scores = get_combined_score(question, final_response, context_str_for_scoring,"ì˜ˆì‹œì‹œ", OPENAI_API_KEY)
                     st.markdown("### GPT í‰ê°€ ì ìˆ˜:")
                     st.json(combined_scores) # Display scores as JSON

            # Show context documents used for the final answer
            with st.expander("ì°¸ê³ í•œ ë¬¸ì„œ (ìµœì¢… ë‹µë³€ ìƒì„±ì— ì‚¬ìš©ë¨)"):
                if final_docs_used:
                    for i, doc in enumerate(final_docs_used):
                        st.markdown(f"**ë¬¸ì„œ {i+1}:**")
                        st.markdown(f"*ì¶œì²˜: {doc.metadata.get('file_name', 'N/A')} (ì¸ë±ìŠ¤: {doc.metadata.get('index', 'N/A')})*")
                        st.text_area(label=f"ë¬¸ì„œ {i+1} ë‚´ìš©", value=doc.page_content, height=150, key=f"final_doc_{i}")
                else:
                    st.write("ì‚¬ìš©ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

        except Exception as e:
            st.error(f"ë‹¨ì¼ ì‹¤í–‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            st.error(traceback.format_exc()) # Show detailed traceback for debugging


    # --- Multi-Model Evaluation Section ---
    st.sidebar.markdown("---")
    st.sidebar.header("ëª¨ë¸ ì¡°í•© í‰ê°€")
    st.sidebar.caption("ì„ íƒëœ ëª¨ë¸ë“¤ì˜ ì¡°í•©ìœ¼ë¡œ RAGë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤. ê° ì¡°í•© ì‹¤í–‰ ì‹œ ìœ„ ì„¤ì • ë° ë¶€ê°€ ê¸°ëŠ¥ ì˜µì…˜ì´ ì ìš©ë©ë‹ˆë‹¤.")

    # Checkboxes for selecting models for evaluation run
    eval_embeddings = st.sidebar.multiselect(
        "í‰ê°€í•  ì„ë² ë”© ëª¨ë¸ ì„ íƒ",
        options=list(AVAILABLE_EMBEDDINGS.keys()),
        default=list(AVAILABLE_EMBEDDINGS.keys()), # Default to all
        help="í‰ê°€ì— ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ë“¤ì„ ì„ íƒí•©ë‹ˆë‹¤. 'openai' ì„ íƒ ì‹œ ë¹„ìš© ë°œìƒ."
    )

    eval_use_reranker = st.sidebar.checkbox("í‰ê°€ ì‹œ reranker ì‚¬ìš©", value=True)
    eval_rerankers = []
    if eval_use_reranker:
        eval_rerankers = st.sidebar.multiselect(
            "í‰ê°€í•  reranker ëª¨ë¸ ì„ íƒ",
            options=AVAILABLE_RERANKERS,
            default=AVAILABLE_RERANKERS # Default to all
        )

    # Evaluation Button
    if st.sidebar.button("ëª¨ë¸ ì¡°í•© í‰ê°€ ì‹¤í–‰") and question:
        st.markdown("---")
        st.header("ëª¨ë¸ ì¡°í•© í‰ê°€ ê²°ê³¼")
        evaluation_results = []

        # Create combinations
        if not eval_embeddings:
             st.warning("í‰ê°€ë¥¼ ìœ„í•´ ìµœì†Œ í•˜ë‚˜ ì´ìƒì˜ ì„ë² ë”© ëª¨ë¸ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
        elif eval_use_reranker and not eval_rerankers:
             st.warning("reranker ì‚¬ìš©ì´ ì„ íƒë˜ì—ˆìœ¼ë‚˜, í‰ê°€í•  reranker ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            # Determine reranker list: either selected ones or [None] if not using reranker
            reranker_list_to_iterate = eval_rerankers if eval_use_reranker else [None]

            st.info(f"{len(eval_embeddings)}ê°œ ì„ë² ë”© ëª¨ë¸ê³¼ {len(reranker_list_to_iterate)}ê°œ ë¦¬ë­ì»¤ ì„¤ì • ì¡°í•©ìœ¼ë¡œ ì´ {len(eval_embeddings) * len(reranker_list_to_iterate)}íšŒ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            st.info(f"ê° ì‹¤í–‰ ì‹œ ë¶€ê°€ ê¸°ëŠ¥ ì„¤ì •ì´ ì ìš©ë©ë‹ˆë‹¤.")

            # Loop through combinations
            for emb_alias in eval_embeddings:
                 # Load vector store for this embedding model (use caching)
                 current_vectorstore = get_cached_vector_store(emb_alias, OPENAI_API_KEY)
                 if not current_vectorstore:
                     st.error(f"í‰ê°€ ì¤‘ë‹¨: '{emb_alias}' ë²¡í„° DB ë¡œë“œ ì‹¤íŒ¨.")
                     evaluation_results.append({
                         "Embedding": emb_alias, "Reranker": "N/A", "Response": "Vectorstore Load Failed",
                         "Fact Check Score": None, "Combined Score": None, "Used Docs": []
                     })
                     continue # Skip to next embedding model

                 for reranker_method in reranker_list_to_iterate:
                     reranker_display_name = 'ì‚¬ìš© ì•ˆ í•¨' if not eval_use_reranker or reranker_method is None else reranker_method
                     run_id = f"Emb: {emb_alias}, Rerank: {reranker_display_name}"
                     st.subheader(f"í‰ê°€ ì‹¤í–‰ ì¤‘: {run_id}")

                     try:
                         # Cost flags for this specific run
                         run_cost_flags = []
                         if emb_alias == 'openai': run_cost_flags.append("OpenAI Emb")
                         if summarize_before_rerank_toggle: run_cost_flags.append("OpenAI Sum")
                         if selected_llm.startswith('gpt'): run_cost_flags.append(f"OpenAI LLM")
                         if use_fact_checker_toggle: run_cost_flags.append("Upstage FC")
                         if use_gpt_scoring_toggle: run_cost_flags.append("OpenAI Score")
                         if run_cost_flags:
                             st.caption(f"API ë¹„ìš© ë°œìƒ ê°€ëŠ¥: {', '.join(run_cost_flags)}")


                         with st.spinner(f"[{run_id}] RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘..."):
                             # Use the core pipeline function
                             eval_response, eval_docs_used = run_rag_pipeline(
                                 question=question,
                                 vectorstore=current_vectorstore,
                                 retriever_k=retriever_k_value, # Use sidebar value
                                 use_reranker=eval_use_reranker, # Specific to eval run
                                 reranker_method=reranker_method, # Current reranker in loop
                                 reranker_top_k=reranker_top_k_value, # Use sidebar value
                                 summarize_before_rerank=summarize_before_rerank_toggle, # Use sidebar value
                                 llm_model_name=selected_llm, # Use sidebar value
                                 openai_api_key=OPENAI_API_KEY,
                                 run_id=run_id
                             )

                         # Optional Fact-Checking for each result (use main toggle)
                         avg_fact_check_score = None
                         if use_fact_checker_toggle and eval_docs_used:
                             with st.spinner(f"[{run_id}] Fact Checking ì¤‘... (Upstage API)"):
                                 sentences = sentence_split(eval_response)
                                 if sentences:
                                     avg_fact_check_score, _ = fact_checker(sentences, eval_docs_used, UPSTAGE_API_KEY)

                         # Optional GPT Scoring for each result (use main toggle)
                         combined_scores = None
                         if use_gpt_scoring_toggle and eval_docs_used:
                             with st.spinner(f"[{run_id}] GPT Scoring ì¤‘... (OpenAI API)"):
                                 context_str = format_docs(eval_docs_used)
                                 
                                 combined_scores = get_combined_score(question, eval_response, context_str,"ì˜ˆì‹œì‹œ", OPENAI_API_KEY)


                         evaluation_results.append({
                             "Embedding": emb_alias,
                             "Reranker": reranker_display_name,
                             "Response": eval_response,
                             "Fact Check Score": avg_fact_check_score,
                             "Combined Score": combined_scores,
                             "Used Docs": eval_docs_used
                         })
                         st.success(f"[{run_id}] ì™„ë£Œ.")

                     except Exception as e:
                         st.error(f"[{run_id}] í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                         evaluation_results.append({
                             "Embedding": emb_alias,
                             "Reranker": reranker_display_name,
                             "Response": f"ì˜¤ë¥˜ ë°œìƒ: {e}",
                             "Fact Check Score": None,
                             "Combined Score": None,
                             "Used Docs": []
                         })

            # Display all eval results
            st.markdown("---")
            st.subheader("ì¢…í•© í‰ê°€ ê²°ê³¼ ìš”ì•½")
            for i, result in enumerate(evaluation_results):
                 st.markdown(f"**{i+1}. Embedding: {result['Embedding']}, Reranker: {result['Reranker']}**")
                 st.markdown(f"**ì‘ë‹µ:**")
                 st.write(result['Response'])
                 if result['Fact Check Score'] is not None:
                     st.caption(f"Fact Check Score: {result['Fact Check Score']:.4f}")
                 if result['Combined Score'] is not None:
                     st.caption(f"Combined Score:")
                     st.json(result['Combined Score']) # Show GPT scores if available
                 with st.expander(f"ì‚¬ìš©ëœ ë¬¸ì„œ ({len(result['Used Docs'])}ê°œ)"):
                     if result['Used Docs']:
                         # Add index 'j' for the inner loop for unique keys
                         for j, doc in enumerate(result['Used Docs']):
                             st.markdown(f"**ë¬¸ì„œ {j+1}:**")
                             st.markdown(f"*ì¶œì²˜: {doc.metadata.get('file_name', 'N/A')} (ì¸ë±ìŠ¤: {doc.metadata.get('index', 'N/A')})*")
                             st.text_area(
                                 label=f"ë¬¸ì„œ {j+1} ë‚´ìš©", # index
                                 value=doc.page_content,
                                 height=150,
                                 key=f"eval_doc_{i}_{j}" # unique key (ê²°ê³¼ index i + ë¬¸ì„œ index j)
                             )
                     else:
                         st.write("ì‚¬ìš©ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")





    ######################## ---   í‰ê°€ì…‹ ì‹¤í—˜ ì½”ë“œ   ---#####################
    st.sidebar.markdown("---")
    st.sidebar.header("ëª¨ë¸ ì¡°í•© í‰ê°€ (í‰ê°€ì…‹ ì‚¬ìš©)")
    st.sidebar.caption("ì—…ë¡œë“œëœ í‰ê°€ì…‹ íŒŒì¼ì˜ ëª¨ë“  ì§ˆë¬¸ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì¡°í•©ë³„ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  í‰ê·  ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.")

    uploaded_eval_file = st.sidebar.file_uploader("í‰ê°€ì…‹ íŒŒì¼ ì—…ë¡œë“œ (.json)", type=['json'], key="eval_file_uploader")

    eval_embeddings_set = st.sidebar.multiselect(
        "í‰ê°€í•  ì„ë² ë”© ëª¨ë¸ (í‰ê°€ì…‹)",
        options=list(AVAILABLE_EMBEDDINGS.keys()),
        default=list(AVAILABLE_EMBEDDINGS.keys()), # Default to all
        help="í‰ê°€ì…‹ í‰ê°€ì— ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ë“¤ì„ ì„ íƒ."
    )

    eval_use_reranker_set = st.sidebar.checkbox("í‰ê°€ì…‹ í‰ê°€ ì‹œ reranker ì‚¬ìš©", value=True)
    eval_rerankers_set = []
    if eval_use_reranker_set:
        eval_rerankers_set = st.sidebar.multiselect(
            "í‰ê°€í•  reranker ëª¨ë¸ (í‰ê°€ì…‹)",
            options=AVAILABLE_RERANKERS,
            default=AVAILABLE_RERANKERS # Default to all
        )

    # Use session state to store results across potential reruns
    if 'evaluation_details' not in st.session_state:
        st.session_state.evaluation_details = {}
    if 'aggregated_scores' not in st.session_state:
        st.session_state.aggregated_scores = {}

    if st.sidebar.button("í‰ê°€ì…‹ìœ¼ë¡œ ëª¨ë¸ ì¡°í•© í‰ê°€ ì‹¤í–‰") and uploaded_eval_file and eval_embeddings_set:
        st.markdown("---")
        st.header("ëª¨ë¸ ì¡°í•© í‰ê°€ ê²°ê³¼ (í‰ê°€ì…‹)")

        evaluation_set = load_evaluation_set(uploaded_eval_file)

        if evaluation_set:
            # Reset previous results before starting a new evaluation
            st.session_state.evaluation_details = {}
            st.session_state.aggregated_scores = {}

            reranker_list_to_iterate_set = eval_rerankers_set if eval_use_reranker_set else [None]

            if eval_use_reranker_set and not eval_rerankers_set:
                st.warning("reranker ì‚¬ìš©ì´ ì„ íƒë˜ì—ˆìœ¼ë‚˜, í‰ê°€í•  reranker ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            else:
                total_combinations = len(eval_embeddings_set) * len(reranker_list_to_iterate_set)
                total_questions = len(evaluation_set)
                st.info(f"{len(eval_embeddings_set)}ê°œ ì„ë² ë”©, {len(reranker_list_to_iterate_set)}ê°œ ë¦¬ë­ì»¤ ì„¤ì •ìœ¼ë¡œ ì´ {total_combinations}íšŒ ì¡°í•© í‰ê°€.")
                st.info(f"ê° ì¡°í•©ë‹¹ {total_questions}ê°œ ì§ˆë¬¸ì— ëŒ€í•´ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤ (ì´ {total_combinations * total_questions}íšŒ RAG ì‹¤í–‰).")
                st.info(f"ê° ì‹¤í–‰ ì‹œ ë¶€ê°€ ê¸°ëŠ¥ ì„¤ì •ì´ ì ìš©ë©ë‹ˆë‹¤.")

                progress_bar = st.progress(0.0)
                progress_text = st.empty()
                runs_completed = 0
                total_runs = total_combinations * total_questions

                for emb_alias in eval_embeddings_set:
                    current_vectorstore = get_cached_vector_store(emb_alias, OPENAI_API_KEY)
                    if not current_vectorstore:
                        st.error(f"í‰ê°€ ì¤‘ë‹¨: '{emb_alias}' ë²¡í„° DB ë¡œë“œ ì‹¤íŒ¨.")
                        runs_completed += len(reranker_list_to_iterate_set) * total_questions
                        progress_bar.progress(min(1.0, runs_completed / total_runs) if total_runs > 0 else 0.0) # Avoid division by zero
                        continue

                    for reranker_method in reranker_list_to_iterate_set:
                        reranker_display_name = 'ì‚¬ìš© ì•ˆ í•¨' if not eval_use_reranker_set or reranker_method is None else reranker_method
                        combination_key = (emb_alias, reranker_display_name)

                        # Initialize lists for this combination in both dicts
                        if combination_key not in st.session_state.aggregated_scores:
                            st.session_state.aggregated_scores[combination_key] = {'fact_check_scores': [], 'combined_scores': [], 'f1_scores': []}
                        if combination_key not in st.session_state.evaluation_details:
                                st.session_state.evaluation_details[combination_key] = []

                        st.subheader(f"ì¡°í•© í‰ê°€ ì¤‘: Emb: {emb_alias}, Rerank: {reranker_display_name}")

                        # Cost flags
                        run_cost_flags = []
                        if emb_alias == 'openai': run_cost_flags.append("OpenAI Emb")
                        if summarize_before_rerank_toggle: run_cost_flags.append("OpenAI Sum")
                        if selected_llm.startswith('gpt'): run_cost_flags.append(f"OpenAI LLM")
                        if use_fact_checker_toggle: run_cost_flags.append("Upstage FC")
                        if use_gpt_scoring_toggle: run_cost_flags.append(f"OpenAI Score (x{total_questions})")
                        if run_cost_flags: st.caption(f"API ë¹„ìš© ë°œìƒ ê°€ëŠ¥: {', '.join(run_cost_flags)}")

                        for i, eval_item in enumerate(evaluation_set):
                            eval_question = eval_item['question']
                            eval_ground_truth = eval_item['answer']
                            run_id = f"Set_Emb:{emb_alias}_Rerank:{reranker_display_name}_Q:{i+1}"

                            progress_text.text(f"ì§„í–‰ë¥ : {runs_completed+1}/{total_runs} - {run_id}")

                            #response ë° score ì„ ì–¸ì–¸
                            eval_response = "ì˜¤ë¥˜"
                            eval_docs_used = []
                            avg_fact_check_score = None
                            avg_f1_score = None
                            combined_scores = None

                            try:
                                with st.spinner(f"[{run_id}] RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘..."):
                                        eval_response, eval_docs_used = run_rag_pipeline(
                                            question=eval_question, vectorstore=current_vectorstore,
                                            retriever_k=retriever_k_value, use_reranker=eval_use_reranker_set,
                                            reranker_method=reranker_method, reranker_top_k=reranker_top_k_value,
                                            summarize_before_rerank=summarize_before_rerank_toggle,
                                            llm_model_name=selected_llm, openai_api_key=OPENAI_API_KEY,
                                            run_id=run_id
                                        )

                                if use_fact_checker_toggle and eval_docs_used:
                                        with st.spinner(f"[{run_id}] Fact Checking ì¤‘..."):
                                            sentences = sentence_split(eval_response)
                                            if sentences:
                                                avg_fact_check_score, _ = fact_checker(sentences, eval_docs_used, UPSTAGE_API_KEY)
                                        if avg_fact_check_score is not None:
                                            # Store for average calculation
                                            st.session_state.aggregated_scores[combination_key]['fact_check_scores'].append(avg_fact_check_score)

                                if use_gpt_scoring_toggle and eval_docs_used:
                                        with st.spinner(f"[{run_id}] Combined Scoring ì¤‘..."):
    #################################### Metadata í˜•ì‹ ################################
    # eval_docs_used = 'plant': 'ìƒì¶”', 'index': 87, 'file_name': 'ìƒì¶”.PDF', 'character_num': 1390

    #í‰ê°€ì…‹ì˜ í˜•ì‹ë§Œ ì•Œë©´ ë¨.
    #question', 'answer', 'p', 'num', 'no', 'name'ë¥¼ keyë¡œ ê°€ì§
    #nameê³¼numì´ ì—†ì„ ê²½ìš° "unrelate"ë¥¼ ì €ì¥í•˜ë„ë¡ í•¨í•¨

    #í‰ê°€ì…‹ ì²­í¬ëŠ” 
    #íŒŒì¼ì´ë¦„ì´ "filtered_plants1.json", "filtered_plants2.json"ì¸ ê²Œ í‰ê°€ì…‹ì˜ nameì˜ FAQ1, FAQ2ì™€ ëŒ€ì‘
    #PDF(ì‘ë¬¼.pdf ë˜ëŠ” ì‘ë¬¼.PDFë¡œ ì €ì¥)ì˜ plantëŠ” í‰ê°€ì…‹ì˜ì˜ nameì— ì €ì¥ë˜ì–´ ìˆë‹¤.

    #eval_docs_usedì— ì €ì¥ëœ íŒŒì¼ ì´ë¦„ì´ FAQ1 or FAQ2ê°€ ì•„ë‹ ê²½ìš°, eval_docs_uisedì˜ plantì™€ index, ê·¸ë¦¬ê³  í‰ê°€ì…‹ ì²­í¬ì˜ nameê³¼ numì„ ë¹„êµí•´ì„œ ë‹¤ ì¼ì¹˜í•˜ëŠ”ê²Œ ìˆìœ¼ë©´ True
    #eval_docs_usedì— ì €ì¥ëœ íŒŒì¼ ì´ë¦„ì´ FAQ1 or FAQ2ì¼ ê²½ìš°, eval_docs_uisedì˜ file_nameê³¼ index, plantì™€ ê·¸ë¦¬ê³  í‰ê°€ì…‹ ì²­í¬ì˜ nameê³¼ num, pë¥¼ ë¹„êµí•´ì„œ ë‹¤ ì¼ì¹˜í•˜ëŠ”ê²Œ ìˆìœ¼ë©´ True

    #ì´ë•Œ F1 scoreë¥¼ êµ¬í•˜ì—¬ë¼

    #ê·¸ë¦¬ê³  nameì— "unrelate"ê°€ ìˆìœ¼ë©´ F1 score ê³„ì‚°ì—ì„œ ëºŒ.
                                            avg_f1_score = compute_evalset_f1(eval_item, eval_docs_used)
                                            #avg_f1_score = 1
                                            if avg_f1_score is not None:
                                                st.session_state.aggregated_scores[combination_key]['f1_scores'].append(avg_f1_score)
                                            
                                            context_str = format_docs(eval_docs_used)
                                            combined_scores = get_combined_score(query=eval_question, response=eval_response, context=context_str, ground_truth=eval_ground_truth, api_key=OPENAI_API_KEY)
                                        
                                        
                                        if combined_scores and "Error" not in combined_scores:
                                            # Store for average calculation
                                            st.session_state.aggregated_scores[combination_key]['combined_scores'].append(combined_scores)

                                # Store detailed results for this question regardless of errors in scoring
                                st.session_state.evaluation_details[combination_key].append({
                                        "question": eval_question,
                                        "ground_truth": eval_ground_truth,
                                        "response": eval_response,
                                        "fact_check_score": avg_fact_check_score, # May be None
                                        "combined_scores": combined_scores, # May be None or contain Error
                                        "used_docs": eval_docs_used
                                    })

                            except Exception as e:
                                st.error(f"[{run_id}] í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                                # Store minimal detail on error
                                st.session_state.evaluation_details[combination_key].append({
                                    "question": eval_question,
                                    "ground_truth": eval_ground_truth,
                                    "response": f"ì˜¤ë¥˜ ë°œìƒ: {e}",
                                    "fact_check_score": None,
                                    "combined_scores": {"Error": str(e)},
                                    "used_docs": []
                                })
                            finally:
                                runs_completed += 1
                                progress_bar.progress(min(1.0, runs_completed / total_runs) if total_runs > 0 else 0.0)

                        progress_text.text(f"ì§„í–‰ë¥ : {runs_completed}/{total_runs} - ì¡°í•© ì™„ë£Œ: Emb: {emb_alias}, Rerank: {reranker_display_name}")

                progress_bar.progress(1.0)
                progress_text.text(f"í‰ê°€ ì™„ë£Œ! ì´ {runs_completed}/{total_runs} ì‹¤í–‰ ì™„ë£Œ.")

                # --- Calculate and Display Average Scores ---
                st.markdown("---")
                st.subheader("í‰ê°€ì…‹ ê¸°ë°˜ ëª¨ë¸ ì¡°í•©ë³„ í‰ê·  ì ìˆ˜")



                avg_results_display = []
                # Use st.session_state for accessing results
                for (emb_alias, reranker_name), scores_data in st.session_state.aggregated_scores.items():
                    num_successful_gpt_runs = len([s for s in scores_data.get('combined_scores', []) if s and "Error" not in s])
                    if num_successful_gpt_runs == 0 and not scores_data.get('fact_check_scores'):
                        avg_results_display.append(f"**Embedding: {emb_alias}, Reranker: {reranker_name}** (ì„±ê³µ ì§ˆë¬¸ ìˆ˜: 0/{total_questions}) - ê²°ê³¼ ì—†ìŒ")
                        avg_results_display.append("---")
                        continue

                    avg_fact_check = np.mean(scores_data['fact_check_scores']) if scores_data['fact_check_scores'] else None
                    avg_f1_score = np.mean(scores_data['f1_scores']) if scores_data['f1_scores'] else None

                    avg_combined_scores = {}

                    if scores_data['combined_scores']:
                        for s in scores_data['combined_scores']:
                            first_valid_score = next((s for s in scores_data['combined_scores'] if s and "Error" not in s), None)
                        
                        if first_valid_score:
                                score_keys = first_valid_score.keys()
                                for key in score_keys:
                                    key_scores = [s[key] for s in scores_data['combined_scores'] if s and "Error" not in s and key in s and s[key] is not None]
                                    if key_scores:
                                        avg_combined_scores[f"Avg. {key}"] = np.mean(key_scores)
                                    else:
                                        avg_combined_scores[f"Avg. {key}"] = None

                    result_line = f"**Embedding: {emb_alias}, Reranker: {reranker_name}** (ì„±ê³µ ì§ˆë¬¸ ìˆ˜: {num_successful_gpt_runs}/{total_questions})"
                    avg_results_display.append(result_line)

                    if avg_fact_check is not None: avg_results_display.append(f"  - Avg. Fact Check Score: {avg_fact_check:.4f}")
                    if avg_f1_score is not None: avg_results_display.append(f"  - Avg. F1 Score : {avg_f1_score:.4f}")
                    if avg_combined_scores:
                            for key, avg_val in avg_combined_scores.items():
                                avg_results_display.append(f"  - {key}: {avg_val:.4f}" if avg_val is not None else f"  - {key}: N/A")
                    
                    
                    avg_results_display.append("---")

                st.markdown("\n".join(avg_results_display))

                # --- Display Detailed Results ---
                st.markdown("---")
                st.subheader("í‰ê°€ì…‹ ê¸°ë°˜ ëª¨ë¸ ì¡°í•©ë³„ ìƒì„¸ ê²°ê³¼")

                # Use st.session_state for accessing details
                if not st.session_state.evaluation_details:
                        st.write("ìƒì„¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                        for (emb_alias, reranker_name), details_list in st.session_state.evaluation_details.items():
                            expander_label = f"Embedding: {emb_alias}, Reranker: {reranker_name} (ì´ {len(details_list)}ê°œ ì§ˆë¬¸)"
                            with st.expander(expander_label):
                                if not details_list:
                                    st.write("ì´ ì¡°í•©ì— ëŒ€í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                                    continue

                                for i, item in enumerate(details_list):
                                    st.markdown(f"**ì§ˆë¬¸ {i+1}:** {item['question']}")
                                    st.markdown(f"**ì •ë‹µ:** {item['ground_truth']}")
                                    st.markdown(f"**AI ì‘ë‹µ:**")
                                    st.write(item['response']) # Use st.write for potentially long answers

                                    # Display scores if available
                                    if item['fact_check_score'] is not None:
                                        st.caption(f"Fact Check Score: {item['fact_check_score']:.4f}")
                                    
                                    if item['combined_scores'] is not None:
                                        if "Error" in item['combined_scores']:
                                            st.error(f"Combined Score Error: {item['combined_scores']['Error']}")
                                        else:
                                            st.caption("Combined Score:")
                                            st.json(item['combined_scores'])

                                    # Display
                                    used_docs = item.get('used_docs', [])
                                    st.markdown("---")
                                    if used_docs:
                                        st.markdown(f"**ì°¸ê³ í•œ ë¬¸ì„œ ({len(used_docs)}ê°œ):**")
                                        for j, doc in enumerate(used_docs):
                        
                                            doc_metadata = getattr(doc, 'metadata', {})
                                            doc_page_content = getattr(doc, 'page_content', 'N/A')

                                            st.markdown(f"**ë¬¸ì„œ {j+1}:**")
                                            st.markdown(f"*ì¶œì²˜: {doc_metadata.get('file_name', 'N/A')} (ì¸ë±ìŠ¤: {doc_metadata.get('index', 'N/A')})*")

                                            # Unique key for the text_area inside
                                            doc_content_key = f"detail_doc_content_{emb_alias}_{reranker_name}_{i}_{j}"

                                            st.text_area(
                                                label=f"ë¬¸ì„œ {j+1} ë‚´ìš©",
                                                value=doc_page_content,
                                                height=100,
                                                key=doc_content_key
                                            )
                                        st.markdown("<br>", unsafe_allow_html=True) # ë¬¸ì„œ ëª©ë¡ ì•„ë˜ì— ì•½ê°„ì˜ ê³µê°„ ì¶”ê°€
                                    else:
                                        st.markdown("**ì°¸ê³ í•œ ë¬¸ì„œ:** ì—†ìŒ") # ë¬¸ì„œê°€ ì—†ì„ ë•Œ í‘œì‹œ
                                    
                                    # st.markdown("---") # Separator between questions

# Display results even if the button wasn't clicked in this run (results might be in session state)
elif st.session_state.evaluation_details:
    st.markdown("---")
    st.subheader("ì´ì „ í‰ê°€ ê²°ê³¼ (í‰ê°€ì…‹ ê¸°ë°˜)")
    st.info("ìƒˆë¡œìš´ í‰ê°€ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ í‰ê°€ì…‹ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'í‰ê°€ì…‹ìœ¼ë¡œ ëª¨ë¸ ì¡°í•© í‰ê°€ ì‹¤í–‰' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

else:
    st.warning("ë²¡í„° DBë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒëœ ì„ë² ë”© ëª¨ë¸ì— ëŒ€í•œ FAISS ì¸ë±ìŠ¤ê°€ '{FAISS_DB_PATH}' ê²½ë¡œì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

st.markdown("---")
st.caption("Powered by LangChain, OpenAI, FAISS, HuggingFace, Upstage & Streamlit")